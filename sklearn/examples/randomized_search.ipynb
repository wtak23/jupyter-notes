{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing randomized search and grid search for hyperparameter estimation\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#example-model-selection-randomized-search-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "Comparing randomized search and grid search for hyperparameter estimation\n",
    "=========================================================================\n",
    "\n",
    "Compare randomized search and grid search for optimizing hyperparameters of a\n",
    "random forest.\n",
    "All parameters that influence the learning are searched simultaneously\n",
    "(except for the number of estimators, which poses a time / quality tradeoff).\n",
    "\n",
    "The randomized search and the grid search explore exactly the same space of\n",
    "parameters. The result in parameter settings is quite similar, while the run\n",
    "time for randomized search is drastically lower.\n",
    "\n",
    "The performance is slightly worse for the randomized search, though this\n",
    "is most likely a noise effect and would not carry over to a held-out test set.\n",
    "\n",
    "Note that in practice, one would not search over this many different parameters\n",
    "simultaneously using grid search, but pick only the ones deemed most important.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pandas import DataFrame as DF\n",
    "import tak.tak as tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some data\n",
    "iris = load_digits()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_prefit = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bootstrap</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>criterion</th>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features</th>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_split</th>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   0  \\\n",
       "bootstrap                                                       True   \n",
       "criterion                                                       gini   \n",
       "max_depth                                                          3   \n",
       "max_features       <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "min_samples_leaf   <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "min_samples_split  <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "\n",
       "                                                                   1  \n",
       "bootstrap                                                      False  \n",
       "criterion                                                    entropy  \n",
       "max_depth                                                        NaN  \n",
       "max_features       <scipy.stats._distn_infrastructure.rv_frozen o...  \n",
       "min_samples_leaf   <scipy.stats._distn_infrastructure.rv_frozen o...  \n",
       "min_samples_split  <scipy.stats._distn_infrastructure.rv_frozen o...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "DF(param_dist).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 1.67 seconds for 20 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.923 (std: 0.007)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 8, 'criterion': 'gini', 'max_features': 4, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.911 (std: 0.006)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 9, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 5, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.905 (std: 0.009)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 1, 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 18.00 seconds for 216 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.934 (std: 0.014)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 3, 'min_samples_split': 1, 'criterion': 'entropy', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.932 (std: 0.016)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.932 (std: 0.014)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini', 'max_features': 10, 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
