{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from tak.tak import myprint\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 Computing CV metrics (cross_val_score + cross_val_predict)\n",
    "- The simplest way to use cross-validation is to call the `cross_val_score` helper function on the estimator and the dataset.\n",
    "- `scores` variable is an nparray of \"scores\" in each cv-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n",
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "print scores\n",
    "\n",
    "# the mean score and the 95% confidence interval of the score estimate\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, the score computed at each CV iteration is the score method of the estimator.\n",
    "- It is possible to change this by using the scoring parameter.\n",
    "- In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96658312,  1.        ,  0.96658312,  0.96658312,  1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validation.cross_val_score(clf, iris.data, iris.target,cv=5, \n",
    "                                          scoring='f1_weighted')\n",
    "scores                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also use other cv strategies using cv iterator\n",
    "- **self-note**: i probably won't use the `cv.ShuffleSplit` class...just stick with Strafieid CV\n",
    "\n",
    "Why?  See this from [link](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit)\n",
    "> Note: contrary to other cross-validation strategies, **random splits do not guarantee that all folds will be different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.cross_validation.ShuffleSplit'>\n",
      "[ 60 116 144 119 108  69 135  56  80 123 133 106 146  50 147  85  30 101\n",
      "  94  64  89  91 125  48  13 111  95  20  15  52   3 149  98   6  68 109\n",
      "  96  12 102 120 104 128  46  11 110 124  41 148   1 113 139  42   4 129\n",
      "  17  38   5  53 143 105   0  34  28  55  75  35  23  74  31 118  57 131\n",
      "  65  32 138  14 122  19  29 130  49 136  99  82  79 115 145  72  77  25\n",
      "  81 140 142  39  58  88  70  87  36  21   9 103  67 117  47] [114  62  33 107   7 100  40  86  76  71 134  51  73  54  63  37  78  90\n",
      "  45  16 121  66  24   8 126  22  44  97  93  26 137  84  27 127 132  59\n",
      "  18  83  61  92 112   2 141  43  10]\n",
      "[ 80 107  90   0  36 112   5  57 102  55  34 128  33  21  73   7  45 129\n",
      " 103 146 120  94  50 134  99 126 114   9  39  97 101  29  81  20  46  51\n",
      "  53  23  27   2  28  37 111  10  84 137 127  43  87  69 144 140  35  76\n",
      "   3  82 145 116  88  44 147   1  93  38  11 115  54  40  18  41  79  24\n",
      "  56  71  13  31  85  70 132 125 123 100  32 104  83 117 118 138  25 110\n",
      "  16  75 109 121  86 139   4  96  14  61  67 149  95  19  72] [ 92 141 130 119  48 143 122  63  26  64  42 108  91  77  22 148   6  65\n",
      "  47  68  60  15 124  58 142  12  59 105  89  78  52 131 113  98  30 136\n",
      "  66 133  49  62  74  17 106   8 135]\n",
      "[111 125 101 143  23 117  50 146   2 110 118 131  17   6 127  67 124  15\n",
      "  56 115 112  71 145 100  38  80 109  44 144 130  35  97  59  48 134  70\n",
      "  65  98 138  55  43 132 103  30 114  34  18 141 139  49  52  74  26  45\n",
      " 126  39   4  11  53 149  79   8   0   5 133  61 135   7  83  99  22  68\n",
      "  82  20  28  86  14  42  32  25  36  92  75  64 142 120  81  58  13 140\n",
      "  72  87 123  93  91 136  51 116  24  94 106  16  63 128 105] [ 85 137  77 108 122 104 121  12  31  96 113 102  60  66 119  62  76  69\n",
      "  46 129   1  10   3  95  21  41  57  47  78  19 107  90   9 147 148  40\n",
      "  29  27  89  37  33  84  54  88  73]\n"
     ]
    }
   ],
   "source": [
    "# Ah, so ShuffleSplit is also an iterable\n",
    "n_samples = iris.data.shape[0]\n",
    "cv = cross_validation.ShuffleSplit(n_samples, n_iter=3,\n",
    "    test_size=0.3, random_state=0)\n",
    "print cv.__class__\n",
    "for train_indices, test_indices in cv:\n",
    "    print train_indices, test_indices\n",
    "# cross_validation.cross_val_score(clf, iris.data, iris.target, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.cross_validation.KFold'>\n",
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n"
     ]
    }
   ],
   "source": [
    "k_fold = cross_validation.KFold(n=6, n_folds=3)\n",
    "print k_fold.__class__\n",
    "for train_indices, test_indices in k_fold:\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation with held out data\n",
    "- data transformations similarly should be learnt from a training set and applied to held-out data for prediction.\n",
    "- A Pipeline makes it easier to compose estimators, providing this behavior under cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "clf.score(X_test_transformed, y_test)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of `pipeline` to simplify things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.cross_validation.ShuffleSplit'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.97777778,  0.93333333,  0.95555556])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "print(cv.__class__)\n",
    "cross_validation.cross_val_score(clf, iris.data, iris.target, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1.1. Obtaining predictions by cross-validation (cross_val_predict)\n",
    "-  cross_val_predict has a similar interface to cross_val_score, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set\n",
    "- Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics.accuracy_score(iris.target, predicted)  = 0.966666666667\n",
      "predicted = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# These prediction can then be used to evaluate the classifier:\n",
    "predicted = cross_validation.cross_val_predict(clf, iris.data,\n",
    "                                               iris.target, cv=10)\n",
    "myprint(metrics.accuracy_score(iris.target, predicted) )\n",
    "myprint(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 CV iterators (KFold, StratifiedKFold, LeaveOneOut, LeavePOut, LeaveOneLabelOut, LeavePLabelOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.3 A note on shuffling\n",
    "- read below...contains important stuffs\n",
    "- http://scikit-learn.org/stable/modules/cross_validation.html#a-note-on-shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About random_state parameter\n",
    "- The random_state parameter defaults to None, meaning that the shuffling will be different every time KFold(..., shuffle=True) is iterated. \n",
    "- However, GridSearchCV will use the same shuffling for each set of parameters validated by a single call to its fit method.\n",
    "- To ensure results are repeatable (on the same platform), use a fixed value for random_state.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
