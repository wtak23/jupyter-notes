{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "There are 3 different approaches to evaluate the quality of predictions of a model:\n",
    "1. **Estimator score method**: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. This is not discussed on this page, but in each estimator’s documentation.\n",
    "2. **Scoring parameter**: Model-evaluation tools using cross-validation (such as cross_validation.cross_val_score and grid_search.GridSearchCV) rely on an internal scoring strategy. This is discussed in the section The scoring parameter: defining model evaluation rules.\n",
    "3. **Metric functions**: The metrics module implements functions assessing prediction error for specific purposes. These metrics are detailed in sections on Classification metrics, Multilabel ranking metrics, Regression metrics and Clustering metrics.\n",
    "\n",
    "## Relevant modules\n",
    "- `sklearn.metrics.make_scorer`\n",
    "- `sklearn.metrics.classification_report`\n",
    "- `sklearn.metrics.confusion_matrix`\n",
    "- `sklearn.metrics.roc_curve`\n",
    "- `sklearn.metrics.roc_auc_score`\n",
    "- [`sklearn.dummy.DummyClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import grid_search, cross_validation, svm\n",
    "from pprint import pprint\n",
    "from pandas import DataFrame as DF\n",
    "from pandas import Series as SR\n",
    "from tak.tak import myprint, pd_underscore, pd_setdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.1. The scoring parameter: defining model evaluation rules\n",
    "\n",
    "For most cases, we just use the following predefined values:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values\n",
    "\n",
    "(I don't think i need to worry about coming up with my own score function, do I?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07475338, -0.16911634, -0.0698804 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, cross_validation, datasets\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clf = svm.SVC(probability=True, random_state=0)\n",
    "cross_validation.cross_val_score(clf, X, y, scoring='log_loss') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## 3.3.1.2. Defining your scoring strategy from metric functions (make_scorer)\n",
    "[`skl.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer), which can take several parameters:\n",
    "\n",
    "- the python function you want to use (my_custom_loss_func in the example below)\n",
    "- whether the python function returns a **score** (greater_is_better=True, the default) or a **loss** (greater_is_better=False). \n",
    "    - If a loss, the output of the python function is negated by the scorer object, conforming to the cross validation convention that scorers return higher values for better models.\n",
    "- for **classification metrics only**: whether the python function you provided requires continuous decision certainties (needs_threshold=True). \n",
    "    - The default value is False.\n",
    "    - any additional parameters, such as beta in an f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.69314718056\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "# here's an example\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    diff = np.abs(ground_truth - predictions).max()\n",
    "    return np.log(1 + diff)\n",
    "\n",
    "# loss_func will negate the return value of my_custom_loss_func,\n",
    "#  which will be np.log(2), 0.693, given the values for ground_truth\n",
    "#  and predictions defined below.\n",
    "loss  = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "\n",
    "ground_truth = [1, 1]\n",
    "predictions  = [0, 1]\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(ground_truth, predictions)\n",
    "\n",
    "print loss(clf,ground_truth, predictions) \n",
    "\n",
    "# score function negates the return value (so that higher is better convention is followed)\n",
    "print score(clf,ground_truth, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.2 Classification metrics (to measure classif. performance)\n",
    "- Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. \n",
    "- Most implementations allow each sample to provide a weighted contribution to the overall score, through the sample_weight parameter.\n",
    "- See here for a list http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2.4. Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.67      0.80      0.72         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takanori/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 2, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics.precision_score(y_true, y_pred) = 1.0\n",
      "metrics.recall_score(y_true, y_pred) = 0.5\n",
      "metrics.f1_score(y_true, y_pred))  = 0.666666666667\n",
      "metrics.fbeta_score(y_true, y_pred, beta=0.5))  = 0.833333333333\n",
      "metrics.fbeta_score(y_true, y_pred, beta=1))  = 0.666666666667\n",
      "metrics.fbeta_score(y_true, y_pred, beta=2)) = 0.555555555556\n",
      "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5) = (array([ 0.66666667,  1.        ]), array([ 1. ,  0.5]), array([ 0.71428571,  0.83333333]), array([2, 2]))\n",
      "\n",
      "precision=[ 0.66666667  0.5         1.          1.        ]\n",
      "recall=[ 1.   0.5  0.5  0. ]\n",
      "threshold=[ 0.35  0.4   0.8 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79166666666666663"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "myprint(metrics.precision_score(y_true, y_pred))\n",
    "myprint(metrics.recall_score(y_true, y_pred))\n",
    "myprint(metrics.f1_score(y_true, y_pred))  \n",
    "myprint(metrics.fbeta_score(y_true, y_pred, beta=0.5))  \n",
    "myprint(metrics.fbeta_score(y_true, y_pred, beta=1))  \n",
    "myprint(metrics.fbeta_score(y_true, y_pred, beta=2)) \n",
    "myprint(metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "print \"\\nprecision={}\\nrecall={}\\nthreshold={}\".format(precision, recall, threshold) \n",
    "\n",
    "average_precision_score(y_true, y_scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2.11. Receiver operating characteristic (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr = [ 0.   0.5  0.5  1. ]\n",
      "tpr = [ 0.5  0.5  1.   1. ]\n",
      "thresholds = [ 0.8   0.4   0.35  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n",
    "myprint(fpr)\n",
    "myprint(tpr)\n",
    "myprint(thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUC SCORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.6. Dummy estimators\n",
    "- When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. \n",
    "- [DummyClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier) implements three such simple strategies for classification:\n",
    "    1. `stratified` generates random predictions by respecting the training set class distribution.\n",
    "    2. `most_frequent` always predicts the most frequent label in the training set.\n",
    "    3. `uniform` generates predictions uniformly at random.\n",
    "    4. `constant` always predicts a constant label that is provided by the user.\n",
    "- A major motivation of this method is `F1-scoring`, when the positive class is in the minority.\n",
    "\n",
    "**Note that with all these strategies, the predict method completely ignores the input data!**\n",
    "\n",
    "[DummyRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor) also implements four simple rules of thumb for regression:\n",
    "1. **mean** always predicts the mean of the training targets.\n",
    "2. **median** always predicts the median of the training targets.\n",
    "3. **quantile** always predicts a user provided quantile of the training targets.\n",
    "4. **constant** always predicts a constant value that is provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To illustrate DummyClassifier, first let’s create an imbalanced dataset:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y[y != 1] = -1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.score(X_test, y_test) = 0.631578947368\n"
     ]
    }
   ],
   "source": [
    "# Next, let’s compare the accuracy of SVC and most_frequent:\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "myprint(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.score(X_test, y_test) = 0.578947368421\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "myprint(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.score(X_test, y_test)  = 0.973684210526\n"
     ]
    }
   ],
   "source": [
    "# We see that SVC doesn’t do much better than a dummy classifier. Now, let’s change the kernel:\n",
    "clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "myprint(clf.score(X_test, y_test) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
